{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHBEb22blCnH",
        "outputId": "e7a5269d-b7e7-4430-e29b-2b9095eed57b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1: Cluster 0\n",
            "Document 2: Cluster 0\n",
            "Document 3: Cluster 0\n",
            "Document 4: Cluster 0\n",
            "Document 5: Cluster 0\n",
            "Document 6: Cluster 1\n"
          ]
        }
      ],
      "source": [
        "#Kmeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "documents = [\n",
        "\"AI and machine learning are advancing rapidly.\",\n",
        "\"The stock market is experiencing a surge in technology stocks.\",\n",
        "\"Natural Language Processing (NLP) is a subset of AI.\",\n",
        "\"Investors are focusing on AI startups.\",\n",
        "\"Climate change affects weather patterns globally.\",\n",
        "\"Renewable energy sources like solar and wind are vital.\"\n",
        "]\n",
        "# Convert text to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(documents)\n",
        "# Apply K-Means\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(X)\n",
        "# Display clusters\n",
        "for i, label in enumerate(kmeans.labels_):\n",
        "  print(f\"Document {i + 1}: Cluster {label}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Named Entity Recognition (NER) System\n",
        "import spacy\n",
        "text = \"\"\"\n",
        "Elon Musk is the CEO of SpaceX and Tesla. He was born in South Africa and is now based in th\n",
        "e United States.\n",
        "\"\"\"\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "# Extract named entities\n",
        "print(\"Named Entities:\")\n",
        "for entity in doc.ents:\n",
        "  print(f\"{entity.text} - {entity.label_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIF4AvJolw7W",
        "outputId": "5e3b548d-8c86-4b1b-f865-b7da24749d39"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities:\n",
            "Elon Musk - PERSON\n",
            "SpaceX - NORP\n",
            "Tesla - ORG\n",
            "South Africa - GPE\n",
            "United States - GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Web Scraping and Content Extraction\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "# Extract title and main content\n",
        "title = soup.find(\"h1\").text\n",
        "content = \" \".join([p.text for p in soup.find_all(\"p\")[:3]]) # First 3 paragraphs\n",
        "print(\"Page Title:\", title)\n",
        "print(\"\\nExtracted Content:\")\n",
        "print(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYdAAsoil4pU",
        "outputId": "02218b4a-1d35-47e8-a221-e45241adc1bd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page Title: Web scraping\n",
            "\n",
            "Extracted Content:\n",
            "Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites.[1] Web scraping software may directly access the World Wide Web using the Hypertext Transfer Protocol or a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis.\n",
            " Scraping a web page involves fetching it and then extracting data from it. Fetching is the downloading of a page (which a browser does when a user views a page). Therefore, web crawling is a main component of web scraping, to fetch pages for later processing. Having fetched, extraction can take place. The content of a page may be parsed, searched and reformatted, and its data copied into a spreadsheet or loaded into a database. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else. An example would be finding and copying names and telephone numbers, companies and their URLs, or e-mail addresses to a list (contact scraping).\n",
            " As well as contact scraping, web scraping is used as a component of applications used for web indexing, web mining and data mining, online price change monitoring and price comparison, product review scraping (to watch the competition), gathering real estate listings, weather data monitoring, website change detection, research, tracking online presence and reputation, web mashup, and web data integration.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Similarity Checker\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "doc1 = \"Artificial intelligence is transforming industries.\"\n",
        "doc2 = \"AI is revolutionizing the industrial world.\"\n",
        "# Calculate similarity\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform([doc1, doc2])\n",
        "similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "print(\"Similarity Score:\", similarity[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6spiP_nl_sD",
        "outputId": "fcd36e79-8b97-4c88-c917-f96918b97a65"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity Score: 0.10163066979112656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Topic Modeling with LDA\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "documents = [\n",
        "\"AI is a growing field of technology.\",\n",
        "\"Machine learning is part of AI.\",\n",
        "\"Climate change impacts the environment.\",\n",
        "\"Renewable energy sources are vital for sustainability.\"\n",
        "]\n",
        "# Convert text to count vectors\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(documents)\n",
        "# Apply LDA\n",
        "lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
        "lda.fit(X)\n",
        "# Display topics\n",
        "print(\"Topics:\")\n",
        "for i, topic in enumerate(lda.components_):\n",
        "  words = [vectorizer.get_feature_names_out()[j] for j in topic.argsort()[-5:]]\n",
        "  print(f\"Topic {i + 1}: {', '.join(words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-kAf7d4mFIl",
        "outputId": "f66e4f64-7995-4b04-9d17-f1ee559bc813"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics:\n",
            "Topic 1: change, climate, impacts, environment, ai\n",
            "Topic 2: sustainability, sources, energy, renewable, vital\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Image Caption Retrieval\n",
        "import cv2\n",
        "from difflib import get_close_matches\n",
        "captions = {\n",
        "\"dog.jpg\": \"A dog sitting on the grass.\",\n",
        "\"cat.jpg\": \"A cat lying on a couch.\",\n",
        "\"car.jpg\": \"A car parked on the street.\"\n",
        "}\n",
        "image_to_search = \"dog.jpg\"\n",
        "matches = get_close_matches(image_to_search, captions.keys(), n=1, cutoff=0.5)\n",
        "caption = captions[matches[0]] if matches else \"No matching caption found.\"\n",
        "print(\"Image Caption:\")\n",
        "print(caption)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH-qLWAYmOIj",
        "outputId": "d513674d-9e54-4f91-974e-31347a0b4fb9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Caption:\n",
            "A dog sitting on the grass.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementation of PageRank Algorithm\n",
        "import networkx as nx\n",
        "# Create a directed graph\n",
        "graph = nx.DiGraph()\n",
        "graph.add_edges_from([(1, 2), (2, 3), (3, 1), (3, 4), (4, 2)])\n",
        "# Calculate PageRank\n",
        "pagerank = nx.pagerank(graph)\n",
        "print(\"PageRank Scores:\")\n",
        "for node, score in pagerank.items():\n",
        "  print(f\"Node {node}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlI33KlomUgs",
        "outputId": "283c4113-4af6-4cd3-8133-964d220e4dfe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PageRank Scores:\n",
            "Node 1: 0.1736\n",
            "Node 2: 0.3326\n",
            "Node 3: 0.3202\n",
            "Node 4: 0.1736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade snscrape\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzNziXiInYGi",
        "outputId": "36745939-cea8-4f30-8e87-8073292c951f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: snscrape in /usr/local/lib/python3.10/dist-packages (0.7.0.20230622)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from snscrape) (2.32.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from snscrape) (5.3.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from snscrape) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from snscrape) (3.16.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->snscrape) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->snscrape) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Social Media Data Retrieval and Analysis\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "\n",
        "# Function to scrape tweets based on a keyword\n",
        "def scrape_tweets(keyword, count=10):\n",
        "    try:\n",
        "        # Create a generator for tweets containing the keyword\n",
        "        tweets = sntwitter.TwitterSearchScraper(f'{keyword}').get_items()\n",
        "\n",
        "        # Collect a specified number of tweets\n",
        "        tweet_list = []\n",
        "        for i, tweet in enumerate(tweets):\n",
        "            if i >= count:\n",
        "                break\n",
        "            tweet_list.append({\n",
        "                \"username\": tweet.user.username,\n",
        "                \"date\": tweet.date,\n",
        "                \"content\": tweet.content,\n",
        "                \"likes\": tweet.likeCount,\n",
        "                \"retweets\": tweet.retweetCount\n",
        "            })\n",
        "\n",
        "        # Display the tweets\n",
        "        for idx, tweet in enumerate(tweet_list, start=1):\n",
        "            print(f\"Tweet {idx}:\")\n",
        "            print(f\"User: {tweet['username']}\")\n",
        "            print(f\"Date: {tweet['date']}\")\n",
        "            print(f\"Content: {tweet['content']}\")\n",
        "            print(f\"Likes: {tweet['likes']}, Retweets: {tweet['retweets']}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "# Input keyword and scrape tweets\n",
        "if __name__ == \"__main__\":\n",
        "    keyword = input(\"Enter a keyword to search for: \")\n",
        "    scrape_tweets(keyword, count=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU-c-rnkmdN2",
        "outputId": "574be286-ceba-4265-98df-bde035b24606"
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a keyword to search for: David Miller\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:snscrape.base:Error retrieving https://twitter.com/search?f=live&lang=en&q=David+Miller&src=spelling_expansion_revert_click: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=David+Miller&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\"))\n",
            "CRITICAL:snscrape.base:4 requests to https://twitter.com/search?f=live&lang=en&q=David+Miller&src=spelling_expansion_revert_click failed, giving up.\n",
            "CRITICAL:snscrape.base:Errors: SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=David+Miller&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=David+Miller&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=David+Miller&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\")), SSLError(MaxRetryError(\"HTTPSConnectionPool(host='twitter.com', port=443): Max retries exceeded with url: /search?f=live&lang=en&q=David+Miller&src=spelling_expansion_revert_click (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 4 requests to https://twitter.com/search?f=live&lang=en&q=David+Miller&src=spelling_expansion_revert_click failed, giving up.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Audio Transcription and Keyword Search\n",
        "!pip install SpeechRecognition\n",
        "import speech_recognition as sr\n",
        "import re\n",
        "# Initialize recognizer\n",
        "recognizer = sr.Recognizer()\n",
        "# Transcribe audio file\n",
        "audio_file = \"sample.wav\" # Replace with actual file\n",
        "with sr.AudioFile(audio_file) as source:\n",
        "  audio = recognizer.record(source)\n",
        "  text = recognizer.recognize_google(audio)\n",
        "print(\"Transcribed Text:\")\n",
        "print(text)\n",
        "# Search for keywords\n",
        "keyword = \"AI\"\n",
        "found = re.search(keyword, text, re.IGNORECASE)\n",
        "print(\"Keyword Found\" if found else \"Keyword Not Found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "collapsed": true,
        "id": "GwyWVmCMnnTE",
        "outputId": "8d6372ea-e969-4379-fd89-bd7e4ec26a02"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.8.30)\n",
            "Downloading SpeechRecognition-3.11.0-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.11.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'sample.wav'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-e896325fb7d2>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Transcribe audio file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0maudio_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sample.wav\"\u001b[0m \u001b[0;31m# Replace with actual file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudioFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecognize_google\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speech_recognition/__init__.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;31m# attempt to read the file as WAV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename_or_fileobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlittle_endian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# RIFF WAV is a little-endian format (most ``audioop`` operations assume that the frames are stored in little-endian form)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwave\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/wave.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(f, mode)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mWave_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mWave_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/wave.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_i_opened_the_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_i_opened_the_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# else, assume it is an open file object already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample.wav'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question-Answering System Using Knowledge Base\n",
        "from difflib import get_close_matches\n",
        "knowledge_base = {\n",
        "\"What is AI?\": \"AI stands for Artificial Intelligence, the simulation of human intelligence in machines.\",\n",
        "\"What is Machine Learning?\": \"Machine Learning is a subset of AI focused on building models that learn from data.\",\n",
        "\"What is NLP?\": \"NLP, or Natural Language Processing, deals with the interaction betweencomputers and human languages.\"\n",
        "}\n",
        "def answer_question(question):\n",
        "  matches = get_close_matches(question, knowledge_base.keys(), n=1, cutoff=0.5)\n",
        "  return knowledge_base[matches[0]] if matches else \"I don't have an answer for that.\"\n",
        "user_question = input(\"Ask a question: \")\n",
        "response = answer_question(user_question)\n",
        "print(\"Answer:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkGd-8Ibn-o9",
        "outputId": "79f42c63-8120-4399-d504-a59775d389e1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a question: What is AI?\n",
            "Answer:\n",
            "AI stands for Artificial Intelligence, the simulation of human intelligence in machines.\n"
          ]
        }
      ]
    }
  ]
}